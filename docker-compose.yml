services:  
  
  open-webui:
    image: ghcr.io/open-webui/open-webui
    container_name: open-webui
    extra_hosts:
      - host.docker.internal:host-gateway
    ports:
      - 3000:8080
    environment:
      - WEBUI_AUTH=False
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/postgres
      - RAG_WEB_SEARCH_ENGINE=searxng
      - SEARXNG_QUERY_URL=http://host.docker.internal:8080/search?q=<query>
      # Comment out below if not using stable-diffusion-webui
      - AUTOMATIC1111_BASE_URL=http://host.docker.internal:7860/ 
      - ENABLE_IMAGE_GENERATION=True
      # The OpenAI variables below are used to configure litellm through it's OpenAI-compatible API
      - ENABLE_OPENAI_API=True
      - OPENAI_API_BASE_URL=http://litellm:4000/v1
      - OPENAI_API_KEY=sk-1234
      - LANGFUSE_PUBLIC_KEY=lf_pk_1234567890
      - LANGFUSE_SECRET_KEY=lf_sk_1234567890
      - LANGFUSE_HOST=http://langfuse:3000
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - postgres
  
  # Comment out the ollama service if you are running directly on your host machine (e.g. to take advantage of Apple silicon)
  # ollama:
  #   image: ollama/ollama
  #   container_name: ollama
  #   ports:
  #     - 11434:11434
  #   volumes:
  #     - ollama:/root/.ollama
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: all
  #   #           capabilities: [gpu]

  litellm:
    build:
      context: ./litellm
    volumes:
     - ./litellm/config.yaml:/app/config.yaml
    command: [ "--config", "/app/config.yaml", "--port", "4000", "--num_workers", "8" ]
    ports:
      - "4000:4000" # Map the container port to the host, change the host port if necessary
    environment:
      DATABASE_URL: "postgresql://litellm:litellm@postgres:5432/litellm"
      STORE_MODEL_IN_DB: "True" # allows adding models to proxy via UI
      LITELLM_MASTER_KEY: "sk-1234" # Your master key for the proxy server. Can use this to send /chat/completion requests etc
      LITELLM_SALT_KEY: "sk-CHANGEME" # Can NOT CHANGE THIS ONCE SET - It is used to encrypt/decrypt credentials stored in DB. If value of 'LITELLM_SALT_KEY' changes your models cannot be retrieved from DB
      ## OPTIONAL ##
      PORT: 4000 # DO THIS FOR RENDER/RAILWAY
      LANGFUSE_PUBLIC_KEY: "lf_pk_1234567890"
      LANGFUSE_SECRET_KEY: "lf_sk_1234567890"
      # Optional, defaults to https://cloud.langfuse.com
      LANGFUSE_HOST: "http://langfuse:3000"
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel:4317"
    # env_file:
    #   - .env # Load local .env file
    depends_on:
      - postgres  # Indicates that this service depends on the 'db' service, ensuring 'db' starts first
    healthcheck:  # Defines the health check configuration for the container
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1" ]  # Command to execute for health check
      interval: 30s  # Perform health check every 30 seconds
      timeout: 10s   # Health check command times out after 10 seconds
      retries: 3     # Retry up to 3 times if health check fails
      start_period: 40s  # Wait 40 seconds after container start before beginning health checks

  langflow:
    image: langflowai/langflow:latest
    ports:
      - 7860:7860
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11434
      
  otel:
    build: ./open-telemetry
    ports:
      - 4317:4317
      - 4318:4318
    volumes:
      - ./open-telemetry/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    command: ["--config=/etc/otel-collector-config.yaml"]
    
  postgres:
    image: postgres:14
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
    volumes:
      - postgres-db:/var/lib/postgresql/data
      - ./langfuse/init-langfuse-db.sql:/docker-entrypoint-initdb.d/init-langfuse-db.sql
      - ./litellm/init-litellm-db.sql:/docker-entrypoint-initdb.d/init-litellm-db.sql
    healthcheck:
      test: ["CMD", "pg_isready","-U", "postgres"]
      start_period: 5s
    restart: always
  
  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    ports:
      - 9099:9099
    environment:
      - LANGFUSE_PUBLIC_KEY=lf_pk_1234567890
      - LANGFUSE_SECRET_KEY=lf_sk_1234567890
      - LANGFUSE_HOST=http://langfuse:3000
      - ENABLE_RAG_WEB_SEARCH=True
      - RAG_WEB_SEARCH_ENGINE="searxng"
      - RAG_WEB_SEARCH_RESULT_COUNT=3
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=10
      - SEARXNG_QUERY_URL="http://searxng:8080/search?q=<query>"
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - ./langfuse/pipelines:/app/pipelines
  
  langfuse:
    image: langfuse/langfuse:2
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "3001:3000"
    environment:
      - DATABASE_URL=postgresql://langfuse:langfuse@postgres:5432/langfuse
      - NEXTAUTH_SECRET=mysecret
      - SALT=mysalt
      - ENCRYPTION_KEY=0000000000000000000000000000000000000000000000000000000000000000 # generate via `openssl rand -hex 32`
      - NEXTAUTH_URL=http://localhost:3000
      - TELEMETRY_ENABLED=${TELEMETRY_ENABLED:-true}
      - LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES=${LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES:-false}
      - LANGFUSE_INIT_ORG_ID=${LANGFUSE_INIT_ORG_ID:-my-org}
      - LANGFUSE_INIT_ORG_NAME=${LANGFUSE_INIT_ORG_NAME:-My Org}
      - LANGFUSE_INIT_PROJECT_ID=${LANGFUSE_INIT_PROJECT_ID:-my-project}
      - LANGFUSE_INIT_PROJECT_NAME=${LANGFUSE_INIT_PROJECT_NAME:-My Project}
      - LANGFUSE_INIT_PROJECT_PUBLIC_KEY=${LANGFUSE_INIT_PROJECT_PUBLIC_KEY:-lf_pk_1234567890}
      - LANGFUSE_INIT_PROJECT_SECRET_KEY=${LANGFUSE_INIT_PROJECT_SECRET_KEY:-lf_sk_1234567890}
      - LANGFUSE_INIT_USER_EMAIL=${LANGFUSE_INIT_USER_EMAIL:-langfuse@example.com}
      - LANGFUSE_INIT_USER_NAME=${LANGFUSE_INIT_USER_NAME:-langfuse}
      - LANGFUSE_INIT_USER_PASSWORD=${LANGFUSE_INIT_USER_PASSWORD:-langfuse}

  searxng:
    image: searxng/searxng:latest
    ports:
      - 8080:8080
    volumes:
      - ./searxng:/etc/searxng
  
  jupyter:
    build: 
      context: ./jupyter
    ports: 
      - 8888:8888
    volumes:
      - ./data:/workspace/data
      - ./jupyter/notebooks/:/workspace/notebooks/
      - ~/.cache/:/root/.cache/
    extra_hosts:
      - host.docker.internal:host-gateway

  # opensearch: # This is also the hostname of the container within the Docker network (i.e. https://opensearch-node1/)
  #   image: opensearchproject/opensearch:latest # Specifying the latest available image - modify if you want a specific version
  #   environment:
  #     - OPENSEARCH_INITIAL_ADMIN_PASSWORD=El@sticSearch123
  #     - OPENSEARCH_USERNAME=opensearch
  #     - OPENSEARCH_PASSWORD=opensearch
  #     - DISABLE_SECURITY_PLUGIN=true
  #     - DISABLE_INSTALL_DEMO_CONFIG=true
  #     - discovery.type=single-node
  #     - bootstrap.memory_lock=true # Disable JVM heap memory swapping
  #     - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx1g" # Set min and max JVM heap sizes to at least 50% of system RAM
  #   ulimits:
  #     memlock:
  #       soft: -1 # Set memlock to unlimited (no soft or hard limit)
  #       hard: -1
  #     nofile:
  #       soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536
  #       hard: 65536
  #   volumes:
  #     - opensearch-data:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container
  #   ports:
  #     - 9200:9200 # REST API
  #     - 9600:9600 # Performance Analyzer
  # opensearch-dashboards:
  #   image: opensearchproject/opensearch-dashboards:latest # Make sure the version of opensearch-dashboards matches the version of opensearch installed on other nodes
  #   ports:
  #     - 5601:5601 # Map host port 5601 to container port 5601
  #   expose:
  #     - "5601" # Expose port 5601 for web access to OpenSearch Dashboards
  #   environment:
  #     OPENSEARCH_HOSTS: '["http://opensearch:9200"]' # Define the OpenSearch nodes that OpenSearch Dashboards will query
  #     DISABLE_SECURITY_DASHBOARDS_PLUGIN: true
  neo4j:
    image: neo4j:latest
    volumes:
        - ./neo4j/logs:/logs
        - ./neo4j/config:/config
        - ./neo4j/data:/data
        - ./neo4j/plugins:/plugins
    environment:
        - NEO4J_AUTH=neo4j/password
        - NEO4JLABS_PLUGINS=["apoc"]
    ports:
      - "7474:7474"
      - "7687:7687"
    extra_hosts:
      - host.docker.internal:host-gateway
  
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.16.1 #8.12.0 #7.17.16
    restart: always
    environment:
      - xpack.security.enabled=false
      - discovery.type=single-node
      - telemetry.optIn=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
  kibana:
    image: docker.elastic.co/kibana/kibana:8.16.1 #8.12.0 #7.17.16
    restart: always
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200    # address of elasticsearch docker container which kibana will connect
    ports:
      - 5601:5601
    depends_on:
      - elasticsearch
  # crawler:
  #   image: docker.elastic.co/integrations/crawler:${CRAWLER_VERSION:-0.2.0}
  #   container_name: crawler
  #   volumes:
  #     - ./crawler/config:/app/config
  #   stdin_open: true   # Equivalent to -i
  #   tty: true                                    # kibana will start when elasticsearch has started

volumes:
  ollama:
  open-webui:
  pipelines:
  postgres-db:
  elasticsearch-data:
  opensearch-data:
  